	Currently, society is becoming more and more technological.  Cars have moved from being entirely mechanical to nearly driving themselves.  Airplanes are now successfully taking off and landing on autopilot.  The question is then raised: “If we can teach computers to accomplish tasks, can we get computers to learn to teach themselves?”  The study of this question has resulted in the development of a field known as Artificial Intelligence (AI).  
	AI is divided into subgroups of study that focus on the following goals: reasoning, knowledge, planning, learning, communication, perception, moving and manipulating objects.  Machine Learning is of special interest in this paper and it explores a computer’s ability to study data, form patterns, and make predictions.  Artificial Neural Networks (ANN) are just one of the many ways this is done.  ANNs can be described simply as having $3$ parts: input layer, hidden layer, output layer.  This type of network is inspired by the biological neural network in the nervous system.  ANNs are comprised of a system of highly interconnected neurons that send messages to each other to solve problems.  The process of learning is achieved through the alterations of synaptic connections.  
	Simply, ANNs have $3$ layers, but of course it isn’t quite as easy as that.  The first layer is the input layer.  This is the data entered into the network.  This information is sent to the hidden layers where the real magic happens.  They are called hidden layers because the building blocks are set up, but the computation is left out of reach.  The hidden layers decide, based on a weighting system, the connections to make to the output layers.  The weighting system is determined by functions that are chosen by the programmer and computed by the network.  These functions can be as simple as logical operators “AND”, “OR”, and “XOR”, but can also be more complicated by using linear, sigmoid, or hyperbolic tangent functions.  The function is generally chosen by the complexity of the problem, though can require some trial and error to determine the best fit.  
	Learning through an ANN can be either supervised or unsupervised.  Supervised learning is done through an external teacher.  The network is told the desired response to certain input signals.  Reinforcement learning is a form of supervised where a correct response is given a certain weight and the better the weight, the more correct the answer.  Unsupervised learning, also known as self-organizing, is based entirely upon local information.  The network is given a large amount of data and allowed to recognize its own patterns.  
	In the previous project, several algorithms that calculated the greatest common divisor (gcd) were tested for efficiency.  The next logical step was to determine whether the efficiency could be improved upon.  AI was chosen as a possible way to do this.  Surely if a computer can figure out how to drive a car, it can learn the relationship between a set of two numbers and finding the greatest common divisor.  The network was set up as a reinforced back-propagation system using Pybrain in Python.  The teaching dataset used was combinations of ($x$,$y$), with $x$ ranging from $2$-$5$ and $y$ ranging from $1$-$20$.  The input was a pair of $x$ and $y$ combined with the desired output of the gcd.  It was set up to randomly go through $10,000$ iterations of the training dataset and then attempt to guess the output for each of the teaching entries.  For the majority of the runs, there were $2$ hidden layers activated.  
	After many executions of the network, several things were determined to have an effect on the output.  When the learning rate was increased from $0.05$ to $0.1$, the results improved slightly, though not drastically. Momentum was determined to work best at the value $0.025$ , much higher and it caused the program to crash, however if it was too low, the program would get stuck in a trough.  Surprisingly, the organization of the training data had some of the most interesting results.  You could almost feed the computer already established patterns to recognize.  Many different combinations were tried with the organization of the output gcd’s.  Generally, whichever two were listed first in the training dataset would be guessed exactly correct.  After those two, it would begin to go wrong very quickly.  Though there were many different ways to produce varying results, none of them were able to produce anything close to the desired results.
	There are several explanations that could be applied to why the network was unable to produce the desired results.  First, the training dataset could have been too small.  There was never more than 60 inputs to find a pattern within.  Sometimes it is difficult to find a pattern in a small section of data.  Though the patterns may have been visible to the human eye, the computer has no prior knowledge of divisors to rely on when trying to find the pattern.  Secondly, it could be related to the fact that prime numbers do not follow a well-defined pattern.  They converge to a distribution, however it is not uniform, therefore it would take an extremely sophisticated machine to recognize that pattern.  This would mean that a pattern would be difficult to find for a pair of numbers where one of them is prime.  Finally, it could be that a neural network is simply not well suited for this type of problem.  There have already been well established algorithms, as demonstrated in the previous project, which rely on more concrete building blocks than just recognizing patterns that may not even be there. 
